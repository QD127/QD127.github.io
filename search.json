[{"path":"/2025/02/24/00Apython数据分析笔记/","content":"title：python数据分析笔记date：2025-02-24tags： - pythonpython数据分析笔记Pycharm的常用快捷键•tab：代码自动补全 •Ctrl + D 复制选定的区域或行到后面或下一行 •Ctrl + Space 基本的代码完成（类、方法、属性） •Ctrl + Alt + Space 快速导入任意类 •Ctrl+F 搜索 •Ctrl+R 替换 •Ctrl+G 寻找 •Ctrl + Shift + Enter 语句完成 •Ctrl + P 参数信息（在方法中调用参数） •Ctrl + Q 快速查看文档 •Ctrl + 鼠标 简介 •Alt + Insert 自动生成代码 •Ctrl + W 选中增加的代码块 •Ctrl + Shift + W 回到之前状态 •Ctrl + Shift + ]&#x2F;[ 选定代码块结束、开始 •Alt + Enter 快速修正 •Ctrl + Alt + L 代码格式化 •Shift + F10 运行 •Shift + F9 调试 •Ctrl + Alt + O 自动导入 •Ctrl + Alt + I 自动缩进 •Shift + F1 外部文档 •Ctrl + F1 显示错误描述或警告信息 Pycharm最全常用快捷键总结_pycharm便捷操作-CSDN博客 迭代器1234567891011121314# 创建一个列表my_list = [1, 2, 3, 4]# 获取迭代器my_iter = iter(my_list)# 使用 next() 函数遍历元素print(next(my_iter)) # 输出: 1print(next(my_iter)) # 输出: 2print(next(my_iter)) # 输出: 3print(next(my_iter)) # 输出: 4# 再次调用 next() 会引发 StopIteration 异常# print(next(my_iter)) # 引发 StopIteration for循环遍历 1234567891011#!/usr/bin/python3# 创建一个列表list = [1, 2, 3, 4]# 创建迭代器对象it = iter(list)# 使用 for 循环遍历迭代器for x in it: print(x, end=&quot; &quot;) next函数 1234567891011#!/usr/bin/python3import sys # 引入 sys 模块list = [1, 2, 3, 4]it = iter(list) # 创建迭代器对象while True: try: print(next(it)) except StopIteration: sys.exit() 生成器1234567891011121314151617# 定义一个生成器函数def countdown(n): while n &gt; 0: yield n n -= 1# 创建生成器对象generator = countdown(5)# 使用 next() 获取值print(next(generator)) # 输出: 5print(next(generator)) # 输出: 4print(next(generator)) # 输出: 3# 使用 for 循环遍历生成器for value in generator: print(value, end=&quot; &quot;) # 输出: 2 1 匿名函数lambda1lambda 参数1, 参数2, ... : 表达式 **lambda**：定义匿名函数的关键字。 参数：可以有一个或多个参数，用逗号分隔。 表达式：函数的返回值，只能是一个表达式，不能包含复杂的逻辑或多行代码。 基本用法 12345# 定义一个匿名函数，计算两个数的和add = lambda x, y: x + y# 调用匿名函数print(add(3, 5)) # 输出: 8 网络协议基础HTTP状态码类别原因短语1XXInformational（信息性状态码）接收的请求正在处理2XXSuccess（成功状态码）请求正常处理完毕3XXRedirection（重定向状态码）需要进行附加操作以完成请求4XXClient Error（客户端错误状态码）服务器无法处理请求5XXServerError（服务器错误状态码）服务器处理请求出错 URL（统一地址访问符）•超文本传输协议（HTTP）的统一资源定位符将从因特网获取信息的五个基本元素包括在一个简单的地址中： •传送协议。 层级URL标记符号(为[&#x2F;&#x2F;],固定不变) 访问资源需要的凭证信息（可省略） 服务器。（通常为域名，有时为IP地址） •端口号。（以数字方式表示，若为HTTP的默认值“:80”可省略） 路径。（以“&#x2F;”字符区别路径中的每一个目录名称） •查询。（GET模式的窗体参数，以“?”字符为起点，每个参数以“&amp;”隔开，再以“&#x3D;”分开参数名称与数据，通常以UTF8的URL编码，避开字符冲突的问题） •片段。以“#”字符为起点 request请求格式requests.get&#x2F;post(url,params,data,headers,timeout,verify,allow_redirects,cookies)url：要下载的目标网页的URLparams：字典形式，设置uRL后面的参数，比如?id&#x3D;123&amp;name&#x3D;xiaomingdata：字典或者字符串，一般用于pOST方法时提交数据headers：设置user-agent、refer等请求头timeout：超时时间，单位是秒verify：True&#x2F;False，否进行HTTPs证书验证，默认是，需要自已设置证书地址allow_redirects：True&#x2F;False是否让requests做重定向处理，默认是cookies：附带本地的cookies数据 get返回网页内容1234import requestsx = requests.get(&quot;https://www.runoob.com&quot;)print(x.status_code)print(x.text) post() 发送 POST 请求到指定 url•requests.post(url, data&#x3D;{key: value}, json&#x3D;{key: value}, args) •url 请求 url。 •data 参数为要发送到指定 url 的字典、元组列表、字节或文件对象。 •json 参数为要发送到指定 url 的 JSON 对象。 args 为其他参数，比如 cookies、headers、verify等 123456# 导入 requests 包import requests# 发送请求x = requests.post(&#x27;https://www.runoob.com/try/ajax/demo_post.php&#x27;)# 返回网页内容print(x.text) BeautifulSoup创建 Beautiful Soup 对象bs4 import BeautifulSoup1234567from bs4 import BeautifulSoup#根据HTML网页字符串创建BeautifulSoup对象soup = BeautifulSoup( html_doc, #HTML文档字符串 &#x27;html.parser&#x27;, #HTML解析器 from_encoding=&#x27;utf8&#x27;\t#HTML文档的编码 ) 访问节点信息·通常是通过URL来定位节点并进行访问得到节点：&lt;a href=&#39;1.html&#39;&gt;Python&lt;/a&gt; 123456#获取查找到的节点的标签名称node.name#获取查找到的a节点的href属性node[&#x27;href&#x27;]#获取查找到的a节点的链接文字node.get_text() 对象Tag:HTML 标签加上里面包括的内容就是 TagNavigableString:获取标签内部的文字 操作123456789101112131415from bs4 import BeautifulSoupwith open(&quot;./test.html&quot;) as fin:\thtml_doc=fin.read() soup=BeautifulSoup(html_doc，&quot;html.parser&quot;)div_node=soup.find（&quot;div&quot;，id=&quot;content&quot;)print(div_node)print(&quot;#&quot;*30)links=div_node.find_all(&quot;a&quot;)for link in links:\tprint(link.name，link[&quot;href&quot;],link.get_textO)img=div_node.find(&quot;img&quot;) 正则表达式正则表达式（Regular Expression，简称 regex 或 regexp）是一种用于匹配和处理文本的强大工具。它通过定义特定的模式，可以快速搜索、替换或提取字符串中的内容。正则表达式在文本处理、数据验证、日志分析等场景中非常常用。 1. 普通字符普通字符（如字母、数字）会直接匹配自身。 示例： 正则表达式 hello 可以匹配字符串 &quot;hello&quot;。 2. 元字符元字符是正则表达式中具有特殊含义的字符。常见的元字符包括： 元字符 描述 . 匹配任意单个字符（除了换行符 ）。 ^ 匹配字符串的开头。 $ 匹配字符串的结尾。 * 匹配前面的字符 0 次或多次。 + 匹配前面的字符 1 次或多次。 ? 匹配前面的字符 0 次或 1 次。 \\d 匹配数字（等价于 [0-9]）。 \\D 匹配非数字字符。 \\w 匹配字母、数字或下划线（等价于 [a-zA-Z0-9_]）。 \\W 匹配非字母、数字或下划线的字符。 \\s 匹配空白字符（包括空格、制表符、换行符等）。 \\S 匹配非空白字符。 [] 匹配括号内的任意一个字符。例如 [abc] 匹配 a、b 或 c。 &#96; &#96; () 分组，将多个字符作为一个整体进行处理。 3. 量词量词用于指定字符的匹配次数。 量词 描述 &#123;n&#125; 匹配前面的字符恰好 n 次。 &#123;n,&#125; 匹配前面的字符至少 n 次。 &#123;n,m&#125; 匹配前面的字符 n 到 m 次。 4. 转义字符如果需要匹配元字符本身，可以使用反斜杠 \\ 进行转义。 示例： 匹配 . 字符：\\. 匹配 * 字符：\\* 示例1. 匹配邮箱地址1[\\w\\.-]+@[\\w\\.-]+\\.\\w+ 解释： [\\w\\.-]+：匹配用户名部分（字母、数字、点、下划线、横线）。 @：匹配邮箱中的 @ 符号。 [\\w\\.-]+：匹配域名部分。 \\.\\w+：匹配顶级域名（如 .com、.cn）。 2. 匹配手机号码11[3-9]\\d&#123;9&#125; 解释： 1：手机号码以 1 开头。 [3-9]：第二位是 3 到 9 之间的数字。 \\d&#123;9&#125;：后面跟着 9 位数字。 3. 匹配 URL1https?://[\\w\\.-]+\\.[a-zA-Z]+ 解释： https?：匹配 http 或 https。 ://：匹配 ://。 [\\w\\.-]+：匹配域名部分。 \\.[a-zA-Z]+：匹配顶级域名。 4. 匹配日期（YYYY-MM-DD）1\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; 解释： \\d&#123;4&#125;：匹配 4 位年份。 -：匹配分隔符 -。 \\d&#123;2&#125;：匹配 2 位月份和日期。 Python 中的正则表达式Python 通过 re 模块支持正则表达式。以下是常用的方法： 1. re.match()从字符串开头匹配正则表达式。 12345678910import repattern = r&quot;hello&quot;text = &quot;hello world&quot;result = re.match(pattern, text)if result: print(&quot;匹配成功:&quot;, result.group())else: print(&quot;匹配失败&quot;) 2. re.search()在字符串中搜索匹配正则表达式的第一个位置。 12345678910import repattern = r&quot;world&quot;text = &quot;hello world&quot;result = re.search(pattern, text)if result: print(&quot;匹配成功:&quot;, result.group())else: print(&quot;匹配失败&quot;) 3. re.findall()返回所有匹配的结果列表。 re.findall(pattern, string, flags&#x3D;0)patten:匹配的正则表达式string:要匹配的字符串。flags:标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。 flags可选值如下re.l使匹配对大小写不敏感re.L做本地化识别（locale-aware）匹配re.M多行匹配，影响^和$re.s使．匹配包括换行在内的所有字符re.U根据Unicode字符集解析字符。这个标志影响\\w，\\W，\\b，\\B.re.X该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 1234567import repattern = r&quot;\\d+&quot;text = &quot;我有 3 只猫和 5 只狗&quot;result = re.findall(pattern, text)print(&quot;匹配结果:&quot;, result) # 输出: [&#x27;3&#x27;, &#x27;5&#x27;] 爬取代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150# -*- codeing = utf-8 -*-from bs4 import BeautifulSoup # 网页解析，获取数据import re # 正则表达式，进行文字匹配`import urllib.request, urllib.error # 制定URL，获取网页数据import xlwt # 进行excel操作#import sqlite3 # 进行SQLite数据库操作findLink = re.compile(r&#x27;&lt;a href=&quot;(.*?)&quot;&gt;&#x27;) # 创建正则表达式对象，标售规则 影片详情链接的规则findImgSrc = re.compile(r&#x27;&lt;img.*src=&quot;(.*?)&quot;&#x27;, re.S)findTitle = re.compile(r&#x27;&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;&#x27;)findRating = re.compile(r&#x27;&lt;span class=&quot;rating_num&quot; property=&quot;v:average&quot;&gt;(.*)&lt;/span&gt;&#x27;)findJudge = re.compile(r&#x27;&lt;span&gt;(\\d*)人评价&lt;/span&gt;&#x27;)findInq = re.compile(r&#x27;&lt;span class=&quot;inq&quot;&gt;(.*)&lt;/span&gt;&#x27;)findBd = re.compile(r&#x27;&lt;p class=&quot;&quot;&gt;(.*?)&lt;/p&gt;&#x27;, re.S)def main(): baseurl = &quot;https://movie.douban.com/top250?start=&quot; #要爬取的网页链接 # 1.爬取网页 datalist = getData(baseurl) savepath = &quot;豆瓣电影Top250.xls&quot; #当前目录新建XLS，存储进去 # dbpath = &quot;movie.db&quot; #当前目录新建数据库，存储进去 # 3.保存数据 saveData(datalist,savepath) #2种存储方式可以只选择一种 # saveData2DB(datalist,dbpath)# 爬取网页def getData(baseurl): datalist = [] #用来存储爬取的网页信息 for i in range(0, 10): # 调用获取页面信息的函数，10次 url = baseurl + str(i * 25) html = askURL(url) # 保存获取到的网页源码 # 2.逐一解析数据 soup = BeautifulSoup(html, &quot;html.parser&quot;) for item in soup.find_all(&#x27;div&#x27;, class_=&quot;item&quot;): # 查找符合要求的字符串 data = [] # 保存一部电影所有信息 item = str(item) link = re.findall(findLink, item)[0] # 通过正则表达式查找 data.append(link) imgSrc = re.findall(findImgSrc, item)[0] data.append(imgSrc) titles = re.findall(findTitle, item) if (len(titles) == 2): ctitle = titles[0] data.append(ctitle) otitle = titles[1].replace(&quot;/&quot;, &quot;&quot;) #消除转义字符 data.append(otitle) else: data.append(titles[0]) data.append(&#x27; &#x27;) rating = re.findall(findRating, item)[0] data.append(rating) judgeNum = re.findall(findJudge, item)[0] data.append(judgeNum) inq = re.findall(findInq, item) if len(inq) != 0: inq = inq[0].replace(&quot;。&quot;, &quot;&quot;) data.append(inq) else: data.append(&quot; &quot;) bd = re.findall(findBd, item)[0] bd = re.sub(&#x27;&lt;br(\\s+)?/&gt;(\\s+)?&#x27;, &quot;&quot;, bd) bd = re.sub(&#x27;/&#x27;, &quot;&quot;, bd) data.append(bd.strip()) datalist.append(data) return datalist# 得到指定一个URL的网页内容def askURL(url): head = &#123; # 模拟浏览器头部信息，向豆瓣服务器发送消息 &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.0.0&quot; &#125; # 用户代理，表示告诉豆瓣服务器，我们是什么类型的机器、浏览器（本质上是告诉浏览器，我们可以接收什么水平的文件内容） request = urllib.request.Request(url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(request) html = response.read().decode(&quot;utf-8&quot;) except urllib.error.URLError as e: if hasattr(e, &quot;code&quot;): print(e.code) if hasattr(e, &quot;reason&quot;): print(e.reason) return html# 保存数据到表格def saveData(datalist,savepath): print(&quot;save.......&quot;) book = xlwt.Workbook(encoding=&quot;utf-8&quot;,style_compression=0) #创建workbook对象 sheet = book.add_sheet(&#x27;豆瓣电影Top250&#x27;, cell_overwrite_ok=True) #创建工作表 col = (&quot;电影详情链接&quot;,&quot;图片链接&quot;,&quot;影片中文名&quot;,&quot;影片外国名&quot;,&quot;评分&quot;,&quot;评价数&quot;,&quot;概况&quot;,&quot;相关信息&quot;) for i in range(0,8): sheet.write(0,i,col[i]) #列名 for i in range(0,250): # print(&quot;第%d条&quot; %(i+1)) #输出语句，用来测试 data = datalist[i] for j in range(0,8): sheet.write(i+1,j,data[j]) #数据 book.save(savepath) #保存# def saveData2DB(datalist,dbpath):# init_db(dbpath)# conn = sqlite3.connect(dbpath)# cur = conn.cursor()# for data in datalist:# for index in range(len(data)):# if index == 4 or index == 5:# continue# data[index] = &#x27;&quot;&#x27;+data[index]+&#x27;&quot;&#x27;# sql = &#x27;&#x27;&#x27;# insert into movie250(# info_link,pic_link,cname,ename,score,rated,instroduction,info)# values (%s)&#x27;&#x27;&#x27;%&quot;,&quot;.join(data)# # print(sql) #输出查询语句，用来测试# cur.execute(sql)# conn.commit()# cur.close# conn.close()# def init_db(dbpath):# sql = &#x27;&#x27;&#x27;# create table movie250(# id integer primary key autoincrement,# info_link text,# pic_link text,# cname varchar,# ename varchar ,# score numeric,# rated numeric,# instroduction text,# info text# )### &#x27;&#x27;&#x27; #创建数据表# conn = sqlite3.connect(dbpath)# cursor = conn.cursor()# cursor.execute(sql)# conn.commit()# conn.close()# 保存数据到数据库if __name__ == &quot;__main__&quot;: # 当程序执行时 # 调用函数 main() # init_db(&quot;movietest.db&quot;) print(&quot;爬取完毕！&quot;) URL管理器定义两个集合放在类的初始化方法中，选择集合这个数据类型，是因为集合的天然去重性，在同一个集合中不会出现一样的数据。未爬取的URL集合，已爬取的URL集合用来存储没有爬取过数据的URL和已经爬取过数据的URL，为防止重复爬取同一个URL的数据 123def __init__(self): self.new_urls = set() #未爬取的URL集合\tself.old_urls = set() #已爬取的URL集合 获取未爬取URL集合的大小123def new_url_size(self): return len(self.new_urls) # 返回未爬取的URL集合的大小 获取已爬取URL的大小12def old_url_size(self): return len(self.old_urls) # 返回已爬取的URL集合的大小 判断是否有未爬取的URL12def has_new_url(self): return self.new_url_size() != 0 # 返回判断后的真假 获取一个未爬取的URL1234def get_new_url(self): new_url = self.new_urls.pop() # 从未爬取的URL集合中获取一个URL并且从原集合中删除 self.old_urls.add(new_url) # 把这条URL放入已爬取的URL集合中 return new_url # 返回这条URL 将新的URL添加到未爬取的URL集合12345def add_new_url(self,url): if url is None: return if url not in self.new_urls and url not in self.old_urls: # 判断URL是否在未爬取的URL集合和已爬取的URL集合中 self.new_urls.add(url) # 若不在就加入未爬取的URL集合中 添加多个URL链接到未爬取的URL集合12345def add_new_urls(self,urls): if urls is None or len(urls) == 0: return for url in urls: self.add_new_url(url) # 调用添加单个URL的方法 示例:爬取文字数据123456789101112131415161718192021222324252627282930313233343536import requestsimport pandas as pdfrom io import StringIOurl = &#x27;https://tianqi.2345.com/Pc/GetHistory&#x27;headers = &#123; &quot;Referer&quot;: &quot;https://tianqi.2345.com/wea_history/54511.htm&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.0.0&quot;&#125;def crawler_table(year,month): params = &#123; &quot;areaInfo[areaId]&quot;: 54511, &quot;areaInfo[areaType]&quot;: 2, &quot;date[year]&quot;: year, &quot;date[month]&quot;: month &#125; response = requests.get(url, params=params, headers=headers) #print(response.status_code) #print(response.text) data = response.json()[&#x27;data&#x27;] df = pd.read_html(StringIO(data))[0] # print(df.head()) return dfdf_list = []for year in range(2011, 2022): for month in range(1, 13): df = crawler_table(year, month) df_list.append(df)pd.concat(df_list).to_excel(&#x27;北京十年间的天气数据.xlsx&#x27;, index=False) 爬取图片1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import osimport reimport requests# import pandas as pdimport bs4url = &quot;https://image.baidu.com/search/index?tn=baiduimage&amp;ps=1&amp;ct=201326592&amp;lm=-1&amp;cl=2&amp;nc=1&amp;ie=utf-8&amp;dyTabStr=MCwxMiwzLDEsMiwxMyw3LDYsNSw5&amp;word=%E5%B0%8F%E5%A7%90%E5%A7%90%E5%9B%BE%E7%89%87&quot;headers = &#123; &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7&quot;, &quot;Referer&quot;: &quot;https://www.baidu.com/&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br, zstd&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,sl;q=0.5&quot;, &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36&#x27;&#125;response = requests.get(url, headers=headers, timeout=10)# print(response.text)response.encoding = &quot;utf-8&quot;print(response.status_code)# print(response.encoding)html = response.textbs = bs4.BeautifulSoup(html, &quot;html.parser&quot;)img_url_list = re.findall(r&#x27;&quot;objURL&quot;:&quot;(.*?)&quot;&#x27;, html)# imgs = bs.find_all(&quot;img&quot;)# print(imgs)# for img in imgs:# src = img[&quot;src&quot;]# # name = img[&quot;alt&quot;]# # print(name, src)# # src = &quot;https://www.sucai999.com&quot; + src# filename = os.path.basename(src)# with open(f&quot;图片/&#123;filename&#125;&quot;, &#x27;wb&#x27;)as f:# resp_img = requests.get(src)# f.write(resp_img.content)img_path = &quot;./图片/&quot;if not os.path.exists(img_path): os.makedirs(img_path)count = 0for img_url in img_url_list: try: res = requests.get(img_url, headers=headers) if res.status_code == 200: count += 1 filename = os.path.join(img_path, f&quot;图片&#123;str(count)&#125;.jpg&quot;) with open(filename, &quot;wb&quot;) as f: f.write(res.content) print(f&quot;成功下载第&#123;count&#125;张图片&quot;, img_url) except Exception as e: pass"},{"title":"来时的路","path":"/2025/02/01/来时的路/","content":"Q：使用hexo和github搭建个s:人网站 A： 使用 Hexo 和 GitHub 搭建个人网站是一个常见的做法，Hexo 是一个快速、简洁的静态博客框架，而 GitHub Pages 提供了免费的静态网站托管服务。以下是详细的步骤： 1. 安装 Node.js 和 Git首先，确保你的系统上已经安装了 Node.js 和 Git。 Node.js: 下载并安装 Node.js Git: 下载并安装 Git 安装完成后，可以通过以下命令检查是否安装成功： 123node -vnpm -vgit --version 2. 安装 Hexo使用 npm 安装 Hexo： 1npm install -g hexo-cli 安装完成后，可以通过以下命令检查 Hexo 是否安装成功： 1hexo -v 3. 初始化 Hexo 项目创建一个新的 Hexo 项目： 12hexo init my-blogcd my-blog 这会在当前目录下生成一个名为 my-blog 的文件夹，里面包含了 Hexo 的基本文件结构。 #原来终端也可以cd过去（大惊） 4. 安装依赖进入项目目录并安装依赖： 1npm install 5. 本地预览启动本地服务器，预览你的网站： 1hexo server 然后在浏览器中访问 http://localhost:4000，你应该可以看到默认的 Hexo 页面。 6. 配置 GitHub Pages首先，在 GitHub 上创建一个新的仓库，仓库名必须为 你的用户名.github.io（例如，如果你的 GitHub 用户名是 john，那么仓库名应该是 john.github.io）。 7. 配置 Hexo 部署在 Hexo 项目的根目录下找到 _config.yml 文件，编辑 deploy 部分： 1234deploy: type: git repo: https://github.com/你的用户名/你的用户名.github.io.git branch: main 8. 安装 Hexo 部署插件安装 hexo-deployer-git 插件： 1npm install hexo-deployer-git --save 9. 生成静态文件并部署生成静态文件并部署到 GitHub Pages： 123hexo cleanhexo generatehexo deploy 10. 访问你的网站部署完成后，访问 https://你的用户名.github.io，你应该可以看到你的个人网站。 11. 自定义你的网站你可以通过编辑 _config.yml 文件来配置你的网站，或者通过安装主题来改变网站的外观。Hexo 社区有很多优秀的主题可供选择，你可以在 Hexo 主题页面 找到你喜欢的主题。 12. 更新内容每次更新内容后，只需重复以下步骤即可： 123hexo cleanhexo generatehexo deploy 13. 使用自定义域名（可选）如果你有自己的域名，可以通过在 GitHub Pages 的设置中添加自定义域名，并在你的域名管理面板中配置 DNS 记录，将域名指向 GitHub Pages。 14. 备份你的 Hexo 项目（可选）为了确保你的 Hexo 项目安全，你可以将整个项目推送到 GitHub 的另一个仓库中进行备份。 12345git initgit remote add origin https://github.com/你的用户名/你的项目名.gitgit add .git commit -m &quot;Initial commit&quot;git push -u origin main 总结通过以上步骤，你已经成功使用 Hexo 和 GitHub Pages 搭建了一个个人网站。你可以随时更新内容、更换主题，甚至使用自定义域名来进一步提升你的网站。"},{"title":"你好世界喵（第一次尝试）","path":"/2025/01/31/你好世界喵/","content":"LSTM（长短时记忆网络） RNN：实际上就是一个带有 记忆的时间序列的预测模型 缺点：梯度消失和梯度爆炸 ↓ LSTM：LSTM（长短时记忆网络）是一种常用于处理序列数据的深度学习模型，与传统的 RNN（循环神经网络）相比，LSTM引入了三个门（ 输入门、遗忘门、输出门）和一个 细胞状态（cell state）， 使得LSTM能够更好地处理序列中的长期依赖关系。 遗忘门：通过x和ht的操作，并经过sigmoid函数，得到0,1的向量，0对应的就代表之前的记忆某一部分要忘记，1对应的就代表之前的记忆需要留下的部分 &#x3D;&#x3D;&#x3D;&gt;代表复习上一门线性代数所包含的记忆，通过遗忘门，忘记掉和下一门高等数学无关的内容（比如矩阵的秩） 输入门：通过将之前的需要留下的信息和现在需要记住的信息相加，也就是得到了新的记忆状态。&#x3D;&#x3D;&#x3D;&gt;代表复习下一门科目高等数学的时候输入的一些记忆（比如洛必达法则等等），那么已经线性代数残余且和高数相关的部分（比如数学运算）+高数的知识&#x3D;新的记忆状态 输出门：整合，得到一个输出&#x3D;&#x3D;&#x3D;&gt;代表高数所需要的记忆，但是在实际的考试不一定全都发挥出来考到100分。因此，则代表实际的考试分数 LSTM确实是可以在一定程度上解决梯度消失和梯度爆炸的问题 ResNet（残差神经网络） 残差神经网络的主要贡献是发现了“退化现象（Degradation）”，并针对退化现象发明了 “直连边&#x2F;短连接（Shortcut connection）” 简单地增加深度，会导致梯度弥散或梯度爆炸。 ResNet论文提出通过数据的预处理以及在网络中使用 BN（Batch Normalization）层来解决 为了解决深层网络中的退化问题，可以人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为残差网络 (ResNets) U-net 语义分割的目的是判断每个像素点的类别，进行精确的分割 eg自动驾驶中的分割任务的分割结果，可以从一张图片中有效的识别出汽车（深蓝色），行人（红色），红绿灯（黄色），道路（浅紫色）等 编码器中的卷积逐步提取特征（深度增加），maxpool降低空间分辨率（高宽减少） 解码器通过上采样（反卷积）高宽加倍，深度减半；通过卷积降低拼接特征的深度 UNet的关键创新是在解码器中引入了跳跃连接（Skip Connection），即将编码器中的特征图 与解码器中对应的特征图进行连接。这种跳跃连接可以帮助解码器更好地利用不同层次的特征信息，从而提高图像分割的准确性和细节保留能力 GAN（生成对抗网络） 生成对抗网络其实是两个网络的组合：生成网络(Generator)负责生成模拟数据；判别网络(Discriminator)负责判断输入的数据是真实的还是生成的。生成网络要不断优化自己生成的数据让判别网络判断不出来，判别网络也要优化自己让自己判断得更准确。二者关系形成对抗，因此叫对抗网络。 *G是一个生成图片的网络 它接收一个随机的噪声z,通过这个噪声生成图片 叫做G(z) *D是一个判别网络 判别一张图片是不是真实的 它的输入参数是x x代表一张图片 输出D(x)代表x为真实图片的概率 如果为1 那就代表100%是真实的图片 如果输出为0 那就代表不可能是真实的图片 在训练过程中 将随机噪声输入生成网络G,得到生成的图片; 判别器接收生成的图片和真实的图片 并尽量将两者区分开来 在这个计算过程中 能否正确区分生成的图片和真实的图片将作为判别器的损失 而能否生成近似真实的图片并使得判别器将生成的个图片判定为真将作为生成器的损失 生成器的损失是通过判别器的输出来计算的 而判别器的输出是一个概率值"},{"title":"Hello World","path":"/2025/01/31/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]